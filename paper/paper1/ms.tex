\documentclass[modern]{aastex631}
\usepackage{hyperref}
\bibliographystyle{aasjournal}
%\turnoffedit
\usepackage[caption=false]{subfig}
\usepackage{booktabs}
\usepackage{censor}
\usepackage{mathtools}
\usepackage{outlines}
\usepackage{amsmath,bm}

\DeclareMathOperator{\resample}{resample}

\def\Teff{T_{\rm eff}}
\def\vsini{v\sin{i}}
\def\kmps{\mathrm{km}\;\mathrm{s}^{-1}}

\begin{document}
\shorttitle{Blas\'e}
\shortauthors{Gully \& Morley}
\title{Transfer learning for \'echelle spectroscopy with \texttt{blas\'e}}

\author{Michael Gully-Santiago}
\affiliation{University of Texas at Austin Department of Astronomy}

\author{Caroline V. Morley}
\affiliation{University of Texas at Austin Department of Astronomy}

\begin{abstract}

    We introduce blas\'e, a framework for transfer learning for high-grasp echelle spectroscopy.  \texttt{blas\'e} produces extensible models that are both physically informed and highly accurate when compared to data. These models can be evaluated quickly for use in likelihood calculations. It works on data with large spectral grasp: simultaneously wide bandwidth and high spectral resolution.


\end{abstract}

\keywords{High resolution spectroscopy (2096), Stellar spectral lines (1630), Astronomy data modeling(1859), GPU Computing (1969), Calibration (2179), Radial Velocity (1332), Maximum likelihood estimation (1901), Deconvolution (1910), Atomic spectroscopy (2099), Stellar photospheres (1237)},

\section{Introduction}\label{sec:intro}

\subsection{Spectral fitting past and present}

Tens of thousands or more individual spectral lines give rise to a sea of undulations that imbue each stellar spectrum with its characteristic appearance.  The identification and understanding of these lines has defined a large category of astrophysics over the last century.  The field grew from by-eye catalogs of stellar templates \citep{1901AnHar..28..129C} to quantifying the role of atomic ionization balance \citep{1925PhDT.........1P}, to modern synthetic forward models including millions or billions of lines \citep{husser13, 2021ApJ...920...85M}.  As technology has improved, our data and models have become more voluminous, more precise, and more complicated.  The mere act of comparing models to observed spectra can now resemble a computational statistics challenge as much as a scientific one.  Here we introduce a new machine-learning-based framework \texttt{blas\'e} aimed at solving computational, statistical, and scientific challenges associated with data-model comparisons for modern astronomical spectroscopy.

The metaphorical holy grail of astronomical spectroscopy is a function that takes in an observed astronomical spectrum and reports back the postition, amplitude, width, and shape of all of its spectral lines, automatically, accurately, and precisely.  The function would go further. It would report back the systemic radial velocity (RV) and rotational broadening ($v\sin{i}$).  Finally the function would---in an \emph{Inception}-like manner---provide what it believes to be the interpretable generating function that produced the data, so that we may gain insights on future examples of this or other stars.  Solving this problem is hard, for at least four reasons.  First, the spectral lines may overlap, and so the assignment of one line may be partially degenerate with the assignment of some other adjacent line.  Second, extremely wide line wings blend into the continuum, such that the placement of the continuum level may become ill-defined.  Third, the extent of line blending and realized line shape depends strongly on the spectral resolution of the spectrograph, the rotational broadening of the star, and possibly the instrumental configuration at the time of observation.  Finally, telluric absorption lines commingle with the astronomical spectral lines of interest, censoring some spectral regions entirely, or partially confounding other lines with chance allignments.

Addressing these and other challenges forms the backbone of \emph{spectral calibration}, an increasingly valuable specialty as the deficits in our models become intolerable with greater data quantity and quality.  Luckily, many scientific applications in astrophysics do not need the technically demanding noise-free template, nor catalog-of-all-spectral-lines.  A few lines suffice.  For those applications, human inspection of isolated lines and semi-automated equivalent width determination have been and will remain adequate.

But many new and important questions in the fields of stars and exoplanets aspire to reach the margins of what the entire dataset can inform.  In particular, data from high-\emph{grasp} \'echelle spectrographs possess simultaneously high spectral resolving power and high bandwidth, yielding tens of thousands or possibly millions of independent spectral resolution elements for each star, substar, or exoplanet.  Those applications that seek to gain signal by ``stacking'' spectral lines or cross-correlating with templates can hypothetically gain huge boosts in the accessible signal-to-noise ratio compared to a single or few lines.  Most manual and semi-automated methods cannot take advantage of the entire spectral bandwidth, or rely on exact knowledge of the underlying templates and may fail to achieve the hypothetical promise of these high-bandwidth spectrographs \citep{2020AJ....160..198H}.

For example, exoplanet cross-correlation spectroscopy \citep{2013MNRAS.436L..35B} hinges on accurate molecular spectral templates to detect and characterize the atmospheres of exoplanets.  Imperfections in these templates can mute the perceived signal strength of these atmospheric features \citep{2015A&A...575A..20H}.

In extreme precision radial velocity (EPRV) applications, cross-correlation methods work \citep{2018A_A...620A..47D}, but have many limitations \citep{2022arXiv220110639Z}.  Addressing telluric absorption at the $\mathrm{cm/s}$ level may require joint modeling of the star and the Earth's atmospheric absorption before convolution with an instrumental kernel.  This telluric joint modeling capability does not yet exist at a precision that can meet these strenuous demands.

In the case of Doppler imaging, an accurate underlying template is needed to detect longitudinally symmetric structures \citep{1983PASP...95..565V,2021arXiv211006271L} such as polar spots \citep{roettenbacher16} or zonal bands \citep{Crossfield14,2021ApJ...906...64A}.

Existing open-source frameworks have overcome some of these challenges, or have been purpose-built for specialized applications.
These frameworks include \texttt{ROBOSPECT} \citep{2013PASP..125.1164W}, \texttt{specmatch} \citep{2015PhDT........82P}, \texttt{specmatch-emp} \citep{2017ApJ...836...77Y}, \texttt{wobble} \citep{2019AJ....158..164B}, \texttt{starfish} \citep{czekala15}, \texttt{sick} \citep{2016ApJS..223....8C}, \texttt{psoap} \citep{2017ApJ...840...49C}, \texttt{FAL} (Cargile et al. \emph{in prep}), CHIMERA \citep{2015ApJ...807..183L}, the \texttt{Cannon} \citep{2017ApJ...836....5H},  \texttt{MINESweeper} \citep{2020ApJ...900...28C}, and recently \texttt{ExoJAX} \citep{2022ApJS..258...31K}.
The design of these frameworks necessarily have to make a choice in the bias-variance tradeoff: is the tool more \emph{data}-driven or more \emph{model}-driven?  The statistical tradeoff can be viewed as a concession in physical self-consistency for model flexibility: more or fewer parameters; more \emph{accurate} or more \emph{precise}.

A key new enabling technology breaks these classical tradeoffs in data-model comparisons for astronomical spectroscopy.  Automatic differentiation \citep[``autodiff'',][]{2015arXiv150205767G} and its affiliated backpropagation algorithm has revolutionized machine learning and neural network architecture design, and is increasingly applied in astrophysical data analysis contexts, \emph{e.g.} kernel phase coronography with \texttt{poppy} \citep{2021ApJ...907...40P}, and exoplanet orbit fitting with \texttt{exoplanet} \citep{2021JOSS....6.3285F}.  Of the spectroscopy frameworks mentioned above, the \texttt{TensorFlow}-based \citep{tensorflow2015-whitepaper} \texttt{wobble} and the \texttt{JAX}-based \citep{jax2018github} \texttt{ExoJAX} employ autodiff technology.  \texttt{wobble} treats each pixel as a tunable control point, producing $\sim10^5$ parameters for a modern stellar spectrum.  The \texttt{ExoJAX} framework has only $\sim$dozens of tunable parameters that describe the fundamental physical properties controlling a brown dwarf atmosphere.  These two autodiff-aware frameworks span the extreme ends of non-parametric and parametric modeling for spectroscopy.

In this paper we show that autodiff-aware semi-empirical models offer an appealing middle ground: informed from self-consistent models, but refined with data.  In Section \ref{methodology} we introduce the \emph{blas\'e} forward-model design and its \texttt{PyTorch}-based \cite{2019arXiv191201703P} implementation.  We show how to supplant the computationally expensive radiative transfer by cloning pre-computed synthetic spectral models.  We show how to combine both stellar and telluric models in a joint modelling approach.  In Section \ref{transferLearn} we describe how to fine-adjust the cloned models to real-world data from high-grasp near-IR spectrographs, using a transfer-learning step.
%Section \ref{secInjRec} exhibits injection-recovery tests on simulated data of known properties, revealing the scalability and fundamental limits of the technique.  
Finally we discuss many conceivable extensions to the framework, and directions for future work in Section \ref{secDiscuss}.


\section{Methodology I: Cloning stellar spectra}\label{methodology}

\subsection{Overall Architecture and Design Choices}

We start with a high resolution pre-computed synthetic stellar model spectrum, $\mathsf{S}_{\rm abs}(\bm{\lambda})$ at its native resolution sampling and with its original absolute physical flux units. The procedure that follows is largely agnostic to the exact details of how this spectrum was made, or what physics or chemistry it may represent. For the purposes of this paper, we will showcase examples from two well-known families of precomputed synthetic astronomical spectra: \texttt{PHOENIX} \citep{husser13} for stellar spectra $(T_{\mathrm{eff}}\in [2300, 10000]\;K)$ and \texttt{Sonora} \citep{2021ApJ...920...85M} for brown dwarf spectra $(T_{\mathrm{eff}}\in [1000, 2300]\;K)$. The framework may also work for precomputed synthetic spectra of exoplanets, supernovae, galaxies, or even further afield such as laboratory physical chemistry.  We place the following demands on the precomputed spectra. They should have sporadic regions of discernable continuum devoid of lines, and the continuum must vary smoothly in wavelength. The spectral lines or pseudo-lines should be resolved, and not sub-sampled. We suspect most stellar spectra meet these criteria, except for the coolest M dwarfs and brown dwarfs. The method can hypothetically handle spectra without discernable continua, but we suspect some modifications to the preprocessing steps would be needed. We truncate the red and blue limits of the precomputed synthetic spectrum to match a high-bandwidth echelle spectrograph, extended with a buffer at the edges of size $\pm \Delta \lambda_{\mathrm{buffer}}$, chosen to account for plausible radial velocity and rotational broadening of real stars. A generous buffer of $v \sin{i} < 500 \;\kmps$ and $|RV|<500 \;\kmps$ yields a typical buffer of about 30~\AA.

The choice of limiting the bandwidth to a region-of-interest around a single echelle spectrograph bandwidth stems from computational constraints. In principle, there is no fundamental limit to the bandwidth one could clone with the method presented here, up to and including the entire precomputed synthetic spectral model bandwidth. We adopt the exact native wavelength sampling with no smoothing or interpolation, yielding a wavelength vector $\bm{\lambda}_S$ with length $N_\mathrm{S}$ equal to the number of pixels within the extents of our region of interest including the buffers.

At this stage, we have the choice of whether to work in linear or log scale flux units. Adopting the log of the flux would ensure that the cloned model possesses only positive flux values, a desirable trait of any physical spectral model. We have implemented both modes in \texttt{blas\'e}, allowing users to choose their preference.  We only narrate the linear flux unit description in the main text of this document, for the sake of clarity, and since most practitioners tend to think of flux in terms of linear flux units.  The data-model comparison step will always take place in linear flux units, so the only operational difference is the behavior for deep and saturated lines.  Appendix \censorbox{XX} lists the equations adjusted to log flux units.

\subsection{Initialization}

We initialize the cloned model with a series of preprocessing steps. We divide the entire spectrum by a black body $\mathsf{B}(\bm{\lambda}_S)$ of the same effective temperature $T_{\mathrm{eff}}$ as the model template. The resulting signal typically possesses a smooth continuum variation. An additional continuum flattening step ensures that subsequent spectral line finding steps get applied uniformly. Here, we identify the continuum regions by the most prominent peaks in the specturm separated by a distance of at least 5,000$-$90,000 pixels from adjacent peaks, yielding between 60 and 5 control points. We then fit an $n_{\mathrm{poly}} = 3^{rd}-15^{th}$ order polynomial to these peaks, evaluate the polynomial on the entire bandwidth $\mathsf{P}(\bm{\lambda}_S)$, and divide the spectrum by this trend. The result should be a flattened ``continuum-normalized'' spectrum familiar to practitioners in high resolution spectroscopy, with the continuum level close to unity.  It is this spectrum that will serve as the centerpiece of subsequent training steps.  We therefore drop any subscript and simply refer to this flattened spectrum as $\mathsf{S}$:

\begin{eqnarray}
    %\mathsf{S} = \frac{\mathsf{S}_{\rm abs}}{\mathsf{B} \; \mathsf{P}}
    \mathsf{S} = \mathsf{S}_{\rm abs}/\mathsf{B} / \mathsf{P}
    \label{eqnFlattening}
\end{eqnarray}


where the division indicates element-wise division of these arrays or ``vectors'' of flux values.  Visual inspection of this continuum-flattened spectrum typically possesses variations at the $<1\%$ level. Tuning the pixel distance and polynomial order can lower the undulations to $<0.05\%$ level for some input spectra. We found a pixel distance of 50,000 and $3^{rd}$ order polynomial worked across the range of effective temperatures for HPF's bandwidth. The IGRINS spectra had more significant continuum opacity and band heads to accommodate and therefore needed to capture finer-scale variation: 5,000 pixels and a $9^{th}$ order polynomial appeared adequate over most of the effective temperature range.

This high-pass filtering step should be set to capture the genuine spectral shape, without over-fitting broad line-wings such as those in deep Hydrogen and sodium lines. We have experimentally found it helpful to hand-tune models of exceptionally broad and non-standard line wings with an optional pre-processing step that depends on the wavelength range of interest. Here we sketch the approach for common broad lines found in the long-wavelength visible and near-infrared wavelength regions.

% TODO We pre-assign spectral lines for the conspicuous Hydrogen lines and neutral metal lines. Table

We emphasize that a recreation of the unvarnished input spectrum can be obtained by multiplying the continuum-flattened signal by the ``perturbed black body'', $\mathsf{B}(\bm{\lambda}_S)\odot \mathsf{P}(\bm{\lambda}_S)$, that symbolizes the black body modulated by continuum opacity or broad-band radiative transfer effects. This smooth spectrum may be useful for applications that need to keep track of broad-band flux, such as low-resolution spectra, or regions with molecular band heads. The ``perturbed black body'' continuum model contains $n_{\mathrm{poly}}+1$ fixed-but-possibly tunable lookup parameters, plus the fixed input $T_{\mathrm{eff}}$.

Next, we identify the spectral lines. We apply a local-minimum-finding algorithm on the spectrum by defining a prominence threshold $P_{\rm rom} \in (0.005, 0.02)$. This threshold dictates the number of lines that will be modeled: a lower prominence finds more, weaker lines, and a larger prominence finds fewer, deeper lines. The prominence algorithm successfully finds lines that reside on top of broad line wings, or unresolved band heads provided that the individual lines exceed the prominence threshold in their local region. The number of lines $N_{\mathrm{lines}}$ depends on the bandwidth, prominence, and the intrinsic properties of the input spectrum, principally effective temperature and metallicity.

For this paper, we illustrate examples for two \'echelle spectrographs with particularly large spectral grasp: the Habitable Zone Planet Finder \citep[HPF,][]{2014SPIE.9147E..1GM} on the Hobby-Eberly Telescope at McDonald Observatory in Fort Davis, Texas; and the Immersion Grating Infrared Spectrograph \citep[IGRINS,][]{park14}
currently on the Gemini South Telescope on Cerro Pach\'on in Chile. The $R=55,000$ HPF has a native bandwidth of $8079-12785$~\AA, which we expand to $8049-12815$~\AA~including the edge buffers. IGRINS has two cameras for $H$ and $K$ band, with the combined spectrum spanning $14267-25217\;$\AA~ including the edge buffers and the region in-between the two cameras, all at a resolving power of $(R=45,000)$. The spectrograph acquisition, reduction, and post-processing steps yield data $\mathsf{D}(\bm{\lambda}_{D})$, where $\bm{\lambda}_{D}$ is the wavelength vector at the instrumental resolution and sampling of each instrument, generally much coarser than the resolution and sampling grid of the precomputed synthetic spectra. The data wavelength vector may also contain gaps between \'echelle orders, whereas the precomputed wavelength coordinates are usually contiguous. HPF may have up to $2048\times28=$57,344 pixels, and IGRINS has typically about 75,000 pixels, after common trimming of noisy edge pixels and unusable telluric regions.


\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/N_lines_vs_Teff_prom.png}
    \caption{Number of prominent spectral lines versus effective temperature for PHOENIX models truncated to IGRINS (\emph{blue, connected} points) and HPF (\emph{orange, free-standing} points) bandwidths, for different prominence thresholds of $0.02$, $0.01$, and $0.005$.}
    \label{fig_Nlines_vs_teff}
\end{figure}


Figure \ref{fig_Nlines_vs_teff} shows how the number of detected lines $N_{\mathrm{lines}}$ scales with effective temperature and prominence threshold $P_{\rm rom}$ for the \texttt{PHOENIX} grid, truncated to the bandwidths-plus-buffers for HPF and IGRINS. We see between about 2,000 and 20,000 lines depending on the $T_{\mathrm{eff}}$ and $P_{\rm rom}$. HPF and IGRINS have a comparable number of lines, and halving the prominence increases the number of lines by about $20-30\%$ in these ranges. The number of lines monotonically increases towards cooler effective temperatures.
The HPF-truncated spectra have $N_s=$335,849 native resolution samples, comparable to the IGRINS-truncated spectra, $N_s=$330,052.

So far we have only one piece of information about the peaks: their location. Next, we derive coarse properties about each detected peak: its amplitude and width, again using the prominence algorithms implemented in \texttt{scipy} \citep{2020SciPy-NMeth}.

There does not exist a general-purpose, single-shot algorithm for obtaining the lineshape in the presence of overlapping spectral lines: where do the wings of one line begin and the wings of another adjacent line end? We, therefore, do not attempt to determine anything about the lineshape at this stage and instead assume that the lines resemble a Voigt profile, with a guess width about equally split between Lorentzian and Gaussian.

\subsection{The \texttt{blas\'e} Stellar Clone Model}

We have now arrived at the \texttt{blas\'e} clone model $\mathsf{S}_{\rm clone}(\bf{\lambda}_s)$ for a flattened synthetic spectrum $\mathsf{S}$: it is the cumulative product of transmission through the sea of all overlapping spectral lines:

\begin{eqnarray}
    \mathsf{S}_{\rm clone} = {\displaystyle \prod_{j=1}^{N_{\mathrm{lines}}} 1-a_j \mathsf{V}_j } \label{equation1}
\end{eqnarray}

where $\mathsf{V}_j$ is the Voigt profile $\mathsf{V}(\bm{\lambda}_S-\lambda_{\mathrm{c},j}, \sigma_j, \gamma_j)$ with Gaussian standard deviation $\sigma$, Lorentzian half-width $\gamma$, at line center position $\lambda_c$, for the $j^{th}$ spectral line. The amplitude $a$ is always expected to be positive for absorption lines.

The Voigt profile $\mathsf{V}(\lambda, \sigma_j, \gamma_j)$ can be computed in exact closed-form using the Voigt-Hjerting function \citep{1938ApJ....88..508H} as the real part of the complex Fadeeva function \citep[\emph{e.g.}][]{2011arXiv1106.0151Z}. Evaluation of the Fadeeva function can be computationally costly, and so approximate forms may be desirable. Here we adopt the pseudo-Voigt approximation \citep{Ida:nt0146}.


\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/blase_flowchart01.png}
    \caption{Visual flowchart of \texttt{blas\'e}.  The technique starts with a precomputed synthetic stellar spectrum and a precomputed synthetic telluric spectrum with properties close to those previously estimated for target.  Both the stellar and telluric spectra get cloned.  The stellar model is warped to its extrinsic properties, and then the stellar and telluric models get multiplied together.  Next this joint model is convolved with an instrumental kernel and resampled to the data spectrum.  Finally the joint model parameters are optimized to match the data, subject to regularization (not shown).  The final products are semi-empirical super-resolution stellar and telluric templates, the $RV$ and $v\sin{i}$ of the star, and the line amplitude, location, width, and shape of all 10,000+ stellar and telluric lines. \authorcomment1{Some of the spectra are placeholders until we re-run the end-to-end-fitting.  Feedback on this figure layout is sought.} }
    \label{blase_flowchart}
\end{figure}

\subsection{Goodness of fit metric}
The model evaluated with its coarse initial values would have terrible performance: it would only vaguely resemble the synthetic spectral model, with up to $\pm 50\%$ undulations from the inexact assignment of widths, lineshapes, and amplitudes. Instead, we tune the parameters of the model, starting from these coarse initial values. This model has $N_{\mathrm{lines}}\times 3$ free parameters, where the center wavelength is held fixed and the amplitude, width, and lineshape are allowed to vary. We minimize a scalar ``goodness-of-fit'' metric, \emph{aka} loss scalar $\mathcal{L}$, chosen as the mean squared error (MSE), which is proportional to $\chi^2$, the sum of the squares of the residual vector $\mathsf{R} \equiv \mathsf{S}-\mathsf{S}_{\rm clone}$ but has no notion of per-pixel noise since the precomputed synthetic spectrum has no uncertainty:

\begin{eqnarray}
    \mathcal{L} = \sum_i^{N_S} (S_i - S_{\mathrm{clone},i})^2 = \mathsf{R^\intercal}\cdot \mathsf{R} \label{simpleLikelihood}
\end{eqnarray}


As seen in Figure \ref{fig_Nlines_vs_teff}, the number of lines can exceed 7,000, meaning the model has over 7,000 $\times 3 =$ 21,000 free parameters. Fitting that large number of parameters is difficult with conventional optimizers.  Here we employ a variant of Stochastic Gradient Descent (SGD), an optimization technique that can scale to a virtually unlimited number of parameters \citep{2016arXiv160904747R}. This technique computes the derivative of the loss scalar with respect to each of the parameters, the so-called \emph{Jacobian}: $(\frac{\partial \mathcal{L}}{\partial a_j}, \frac{\partial \mathcal{L}}{\partial \sigma_j}, \frac{\partial \mathcal{L}}{\partial \gamma_j})$. The Jacobian indicates how the MSE would decrease with a change in the parameter-of-interest, or put simply which-way and by-how-much you have to change the line properties to get a better fit.

The optimizer updates the $a_j, \sigma_j, \gamma_j$ parameters by a small fraction of the Jacobian---called the learning rate LR---towards the direction that would improve the fit, for all $N_{\mathrm{lines}} \times 3$ parameters simultaneously. The Jacobian is calculated behind the scenes with automatic differentiation implemented as the so-called backpropagation algorithm or simply ``backprop'' \citep{2015arXiv150205767G}. We choose the \texttt{PyTorch} framework that computes these Jacobians efficiently for all of the mathematical primitives in our \texttt{blas\'e} implementation.


\subsection{GPU and Autodiff specific considerations}
We make a few tweaks to the implementation for numerical purposes. First, we want all the parameters to be positive, forbidding negative amplitudes and negative widths. We, therefore, tune the natural log of the parameters and exponentiate them before inclusion in Equation \ref{equation1}. Second, we found through iterative experimentation that the initialization amplitudes and widths were systematically shifted from the optimized values. We built-in scalar tweaks to the initialization amplitudes, Gaussian width, and Lorentzian width, which dramatically accelerated the optimization step.

The autodiff machinery has a convenient way to set which parameters are held fixed and which are iteratively fine-tuned.  One simply disables the Jacobian computations for the fixed parameters.  We set the \texttt{requires\_grad=True} property for any Torch tensor that we want to vary. This allows us to easily explore whether, say, allowing the $\lambda_\mathrm{c}$ parameter to vary significantly improves the fit.

The computational bottleneck occurs at the evaluation of Equation \ref{equation1}, which can be viewed as having a $N_{\mathrm{lines}}\times N_{S}$ matrix $\bm{\bar{F}}$ assembled by stacking each Voigt absoption profile $\mathsf{V}_j(\bm{\lambda}_s)$ on top of each other:

\begin{equation}
    \begin{pmatrix}
        1 - a_1 \mathsf{V}_1(\bm{\lambda}_s)                                       & \\
        1 - a_2 \mathsf{V}_2(\bm{\lambda}_s)                                       & \\
        \vdots                                                                     & \\
        1 - a_{N_{\mathrm{lines}}} \mathsf{V}_{N_{\mathrm{lines}}}(\bm{\lambda}_s) &
    \end{pmatrix}
\end{equation}

An element of this matrix, $F_{ji}$, will have the flux value for a given line at a given wavelength coordinate. Equation \ref{equation1} performs a type of matrix contraction, turning a $N_{\mathrm{lines}}\times N_{S}$ matrix into a length $N_{S}$ row vector. The number of Floating Point Operations (FLOPS) scales with the number of entries in this matrix. We can rewrite Equation \ref{equation1} as a sum by taking the log of both sides and dropping in this $\bm{\bar{F}}$ matrix:

\begin{eqnarray}
    \ln{\mathsf{S}_{\rm clone}} = \sum_{j=1}^{N_{lines}} \ln{F_{ji}} = \mathbf{1} \cdot \ln{\bm{\bar{F}}}
\end{eqnarray}


where $\mathbf{1}$ is a $1\times N_{\rm lines}$ row vector of all-ones. We re\"emphasize that each spectral line has to be painstakingly evaluated across the entire spectral bandwidth.   Efficient GPU algorithms exist for voluminous matrix manipulations such as this one, so this computation can proceed as quickly as possible on modern machines. In particular, the proprietary CUDA architecture for NVIDIA\textsuperscript{\tiny\textregistered} GPUs contains Tensor cores with specialized matrix math. The chief bottleneck occurs when the storage of the $\bm{\bar{F}}$ matrix exceeds the available RAM of a GPU or CPU: the computation will fail with an ``Out of Memory'' exception. Modern NVIDIA GPUs have $8-40$ GB of RAM, which translates roughly to a few thousand spectral lines across $\sim$300,000 pixels.  The memory bottleneck is even more pernicious than mere storage of the $\bm{\bar{F}}$ matrix since the CPU/GPU also has to store the computation graph for autodiff. It is generally not possible to evaluate Equation \ref{equation1} in its entirety in one-fell-swoop. A remedy is needed.


\subsection{Sparsity}

The $\ln{\bm{\bar{F}}}$ matrix is sparsely populated: most of the entries far from the line center are vanishingly close to zero. Here we take advantage of that mostly empty matrix using the mathematics of sparse matrices \citep{saad03:IMS}.

We retain a relatively small number of pixels $N_{\rm cut}$ adjacent to the line center. Setting this wing cut produces a speedup by a factor of $\frac{N_S}{N_{\mathrm{cut}}}$, which can exceed $100\times$ for wide bandwidth spectra.The choice of $N_{\rm cut}$ is nuanced.  It should be set large enough that truncation effects are not seen for the broadest lines.  But even more, $N_{\rm cut}$ has to be future-proofed for Doppler-shifting. Extreme Doppler shifts could hypothetically send line cores entirely outside the extents of $N_{\rm cut}$ if set too low.  We therefore typically set wingcuts comparable to the buffer size $\pm \Delta \lambda_{\mathrm{buffer}}$, even though most weak lines only perceptibly affect $<1\;$\AA. We coerce all wing cuts to be the same number of pixels, typically 6000 pixels for \texttt{PHOENIX}, with the middle pixel being at the line center position, and about 3000 pixels to the red and blue side of the line. We populate a new approximate sparse matrix $\ln{\bm{\hat{F}}}$ with only these $~6000$ pixels per line and assume zeros everywhere else.
%\footnote{The PHOENIX spectra double their pixel sampling shortward of $\lambda = 1 \mu$m, so 6000 pixels in the visible is about 30 Angstroms, but 6000 pixels in the infrared is about 60 \AA. We \emph{could have} demanded that the wing cut be exactly 30 \AA, or some other heuristic, but that choice would result in so-called \emph{jagged} or \emph{ragged} arrays: collections of 1D arrays that differ in their lengths. Some lines would have 5999 pixels, some would have 6000, and others would have 2999 or 3000 pixels, depending whether they occur to the red or blue side of the $1 \;\mu$m kink in sampling. Most efficient CPU and GPU algorithms are not equipped to deal with ragged arrays, therefore increasing the computation time. Instead, the standardization choice of exactly 6000 pixels means we can use standard matrix or tensor arithmetic, which somewhat counterintuitively is faster while handling more pixels for all spectral lines: 6000 pixels everywhere is faster than 6000 pixels for some and 2999 for other lines.  Locating the indices of these 6000 ``nearest-neighbor'' pixels is very fast, only occurs once, and then is fixed.}

The remapping of the sparse matrix can be visualized as the shifting of all lines to the center of this new matrix $\bm{\hat{F}}$.  The algorithmic machinery keeps track of each $(i, j, F_{ji})$ trio of coordinates and flux values.


\begin{equation} \bm{\bar{F}} =
    \begin{pmatrix}
        \cdots \includegraphics[height=1cm]{figures/voigt_01.pdf}  \cdots & \\
        \cdots \includegraphics[height=1cm]{figures/voigt_02.pdf}  \cdots & \\
        \vdots                                                            & \\
        \cdots \includegraphics[height=1cm]{figures/voigt_03.pdf} \cdots  &
    \end{pmatrix} \mapsto
    \begin{pmatrix}
        \includegraphics[height=1cm]{figures/voigt_01b.pdf} & \\
        \includegraphics[height=1cm]{figures/voigt_02b.pdf} & \\
        \vdots                                              & \\
        \includegraphics[height=1cm]{figures/voigt_03b.pdf} &
    \end{pmatrix} = \bm{\hat{F}}
\end{equation}


Sparse matrix methods generally support an operation known as \emph{coalescing}, which sums values with repeated indices.  We sum the logs and then exponentiate the result before comparing it to the native linear flux. We enforce that the argument of the natural log must be greater than zero through a simple truncation, which may approximately resemble the effect of saturation in line cores. This truncation would only be relevant for saturated lines. Each pixel may get computed about $\sim100$ times in this sparse implementation, which is about $50\times$ better than each pixel getting computed $N_{lines}\sim6500$ times in the dense approach.  Efficient algorithms for assembling and coalescing sparse matrices exist in \texttt{PyTorch}



\subsection{Optimization and training}

We use the Adam optimizer \citep{2014arXiv1412.6980K} with a typical learning rate $LR\in (0.005, 0.05)$ and all the defaults for \texttt{PyTorch v1.11}.  We defined the number of training epochs $N_{epoch}=100-10,000$ depending on the application. The user can optionally monitor a live view of the training progress with Tensorboard \citep{tensorflow2015-whitepaper} to gain an intuition for the training efficiency.


\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/blase_clone_demo.png}
    \caption{PHOENIX spectrum cloned with blas\'e.  This $T_{\mathrm{eff}}=4700\;$K,  $\log{g}=4.5$ solar metallicity model has 9,028 individual cloned spectral lines, each with 3 tuned parameters.  The pictured 50 \AA~ chunk contains 121 spectral lines and represents about 1$\%$ of the entire spectral bandwidth that was cloned.  Some flaws can be seen near the cores of deep lines, or wings of broad lines.}
    \label{fig_cloned_spectrum_demo}
\end{figure}

Figure \ref{fig_cloned_spectrum_demo} shows a portion of a PHOENIX spectrum cloned with \texttt{blas\'e}. The $1000$ epochs of training took 56 seconds on an NVIDIA\textsuperscript{\tiny\textregistered} RTX2070 GPU with \texttt{PyTorch v1.11}, \texttt{CUDA v11.1}, and Intel\textsuperscript{\tiny\textregistered} Core\textsuperscript{\tiny TM} i7-9750H CPUs at 2.60GHz, with all tensors as FP64. The same computation on a 2020 M1 Macbook Air took 1$^h$25$^m$ with \texttt{PyTorch v1.9}, $90\times$ slower than the GPU counterpart.

We store the model parameters to disk and refer to the entire collection of parameters as a \emph{pre-trained model}.  More specifically this fine-tuned model represents an evaluable and interpretable clone of the original static pixel values flux values.

\section{Methodology II: Cloning Telluric Spectra}
Ground-based near-IR \'echelle spectra possess countless depressions attributable to molecular line absorption in Earth's atmosphere.  These telluric lines hamper the unbiased interpretation of \'echelle spectra, so some treatment plan is needed.  Often the regions of known, deep tellurics are simply discarded.  In other cases, the lines are modeled with first principles line-by-line radiative transfer \citep[\emph{e.g.} \texttt{TelFit,}][]{2014AJ....148...53G} or through data-driven means \citep[\emph{e.g.} \texttt{wobble,}][]{2019AJ....158..164B}.  The most demanding EPRV applications require a precision characterization of telluric lines that the astronomical community has not yet been able to achieve, and that may rival even the abilities of Earth Science practitioners.  A hybrid data-/model- driven approach was the chief recommendation of the \emph{Telluric Hack Week} workshop aimed at improving our mitigation of the atmosphere's deleterious effects.  The \texttt{blas\'e} framework achieves a key milestone by introducing such a \emph{hybrid} approach.

\subsection{The \texttt{blas\'e} Telluric Clone Model}
We start with a precomputed synthetic telluric model, $\mathsf{T}$ with associated wavelength coordinates $\bm{\lambda}_T$.  We employ a \texttt{TelFit} model, though any precomputed synthetic telluric model will work.  The \texttt{TelFit} model does not contain any continuum sources of opacity, and so we can skip the flattening procedure described in Equation \ref{eqnFlattening}.  We orchestrate the same initialization and line finding as in the stellar models, and obtain a coarse clone.

The number of pixels in the telluric model can be chosen at the time of running a \texttt{TelFit} model.  Here we choose a spectral resolution \censorbox{$R\sim1\times10^6$}, adequate for resolving telluric lines, and yielding \censorbox{xx} pixels across the HPF bandwidth.  This pixel sampling is slightly \censorbox{finer} than the native PHOENIX pixel sampling.  The number of telluric lines depends on the atmospheric properties, in particular the Earth surface temperature and relative humidity $RH$.  For a \censorbox{T=x}, \censorbox{RH=y}, we detect \censorbox{zz} telluric lines across the entire HPF bandwidth.

One guiding principle departs from the stellar case: telluric lines do not require future-proofing for large radial velocity shifts, hypothetically allowing us to reduce the number of pixels needed for a wingcut.  Small radial velocity shifts are possible due to bulk motions in the Earth's atmosphere, but those bulk motions should be much smaller than the speeds of stars towards and away from Earth.  So we can hypothetically tolerate a much smaller wing cut for telluric lines.  In practice, telluric lines can be extremely broad, and accurately cloning these broad telluric lines requires about $N_{\rm cut}\sim6000$ pixels, comparable to the stellar case.

We optimize the sparse telluric clone, achieving comparable computational speed as the PHOENIX cloning task.  We are left with $\mathsf{T}_{\rm clone}(\bm{\lambda}_T)$, the tunable telluric clone model evaluated at its original native coordinates.


\section{Cloning Performance}

We compute the residual $\mathsf{R}(\bm{\lambda}_S)$ of native PHOENIX minus cloned model, illustrated in the bottom panel of Figure \ref{fig_cloned_spectrum_demo}. We see an RMS residuals of 1.2\%/pixel at native resolution. We identify three main categories of cloning flaws.

The first, and expected, source of large residuals is simply missing line opacity due to our finite prominence threshold. Lines with prominence less than $P_{\rm rom}$ yield residual notches with strengths comparable to $P_{\rm rom}$. Including smaller prominence lines by lowering $P_{\rm rom}$ produces smaller residuals, at the tradeoff of computing more lines and yielding higher computational cost.  But at some point, turning down $P_{\rm rom}$ yields diminishing returns, as other imperfections provide a noise floor.  We have experimentally determined this noise floor to occur near $P_{\rm rom}=0.01$.

Second, another anticipated flaw occurs in the line cores of relatively narrow lines, where the pseudo-Voigt profile becomes a poor approximation of the exact Voigt profile.  The cloned model tends to overestimate the flux at the core and underestimate the flux along the slopes of the lines.

Finally, and most perniciously, a large category of residuals appear near the wings of the deepest and broadest lines---such as Hydrogen and neutral alkali metal lines.  The true lines exhibit advanced lineshapes, such as non-Lorentzian line-wings that are not captured with the overly simplistic Voigt line profile.  Figure \ref{fig_zoom_cloning_performance} highlights super-Lorentzian line wings around a line at 8691~\AA.  Narrow lines devolve into missing linewing opacity, the favored tradeoff when the continuum estimate's poor performance outweighs the pain of a narrow-but-tolerably-small spike. This flaw can be seen  where a line initialized at 8692.5~\AA~ and another pair of lines at 8690.0 all melt into linewings.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/blase_clone_demo_zoom.png}
    \caption{Zoom in of the region between the vertical gray bars in Figure \ref{fig_cloned_spectrum_demo}. The cloned model has 7 spectral lines describing the 400 pixels in this 4 \AA~ chunk.  The native PHOENIX pixel sampling can be see as the boxy steps in both the native and cloned model.  }
    \label{fig_zoom_cloning_performance}
\end{figure}

Cloning telluric lines suffers from one additional problem.  Telluric lines can be extremely deep, exhibiting almost vanishing transmission with saturated line cores common in-between the atmospheric windows that define the $I$, $J$, $H$, and $K$- bands.  The \texttt{blas\'e} method cannot cope with these saturated lines, and often it treats nearby blended saturated lines as one single line.  The underfit model flounders.

But overall, the telluric model cloning performs adequately, with \censorbox{X percentage} error-per-pixel overall, and only \censorbox{zz} error-per-pixel when restricted to the atmospheric windows.


\section{Semi-empirical models with transfer learning techniques}\label{transferLearn}


The cloned model already represents a useful intermediate product: the distillation of $N_s=335,849\times2$ pixel flux values and coordinates into a more compact quartet of properties for a list of 9,028 spectral lines, a dimensionality reduction of $18\times$ for the cost of 1.2\%/pixel in accuracy.  But the cloned model serves as a mere stepping stone in our principal quest: the comparison of models to real data.

\subsection{Augmenting the Stellar Clone Model for $RV$ and $v\sin{i}$ }
Real stars possess two key extrinsic properties.  Rotational broadening $v\sin{i}$ and radial velocity $RV$ depend on the observer's viewing location. We follow \citet{czekala15} by emphasizing the qualifier \emph{extrinsic}, to distiguish between stellar \emph{intrinsic} properties, such as $T_{\mathrm{eff}}, \log{g},\mathrm{and\,} [\mathrm{Fe}/\mathrm{H}]$.  Intrinsic properties appear the same from any viewing location---at least for stars with isotropic surfaces---while extrinsic properties do not.  The distinction is important because the extrinsic terms act as simple convolutions and translations to the cloned spectrum.  We therefore define an augmented model, which we designate the ``extrinsic'' model, $\mathsf{S}_{\rm ext}$:

\begin{eqnarray}
    \mathsf{S}_{\rm ext}(\bm{\lambda}_Z) = \mathsf{S}_{\rm clone}(\bm{\lambda}_Z - \frac{RV}{c}\lambda_\mathrm{c}) * \zeta \left(\frac{\bm{v}}{v\sin{i}}\right) \label{eqn_convolution}
\end{eqnarray}

where $\zeta$ is the convolution kernel for rigid body rotation \citep{2022ApJS..258...31K}, $\bm{v}$ is the spectral axis represented as relative velocity coordinates, and $*$ denotes the convolution operator.

Most autodiff-aware \texttt{convolve} operators act in pixel-space, approximating kernels as numerically sampled functions.  There exists a special exponential spectral sample spacing that allows the convolution operators to work out-of-the-box for rotational broadening.  The design of \texttt{blas\'e} permits the telluric model to be re\"evaluated at any wavelength coordinate vector, provided that it adequately samples the underlying lines.  We therefore change the sampling from the native stellar and telluric wavelength coordinate grids, $\bm{\lambda}_S$ and $\bm{\lambda}_T$, to a new exponentially sampled wavelength grid, denoted with the subscript $Z$:

\begin{eqnarray}
    \bm{\lambda}_Z = \lambda_0  \exp{\frac{\bm{v}-v_0}{c}}
\end{eqnarray}

where $\bm{v}-v_0$ is the velocity vector going from zero to the velocity associated with the largest wavelength, with \emph{linear} spaced velocity samples.  We choose a sampling in velocity space of 0.5 km/s, which corresponds to about $10\times$ finer than the instrumental resolving power of HPF, and delivers a minimum and maximum wavelength spacing of 0.013 and 0.024 \AA$/\text{pixel}$ respectively for the HPF bandwidth.

Operationally, the radial velocity shift $RV$ gets applied to the line center positions rather than scaling the entire wavelength grid point coordinates, $\bm{\lambda}_Z$. This approximate choice ignores relativistic effects, but yields a convenience: it cleanly makes the $RV$ autodiff-aware, meaning that an infinitesimal change to the RV value can be sensed through backpropagation by affecting only the line center positions.


\subsection{Joint Stellar and Telluric Model}
We then simply multiply the stellar model by the telluric transmission:

\begin{eqnarray}
    \mathsf{M}_{\rm joint} = \mathsf{S}_{\rm ext}(\bm{\lambda}_Z) \odot \mathsf{T}_{\rm clone}(\bm{\lambda}_Z)
\end{eqnarray}

It is only at this stage that we may apply the instrumental broadening kernel.  The instrumental resolving power, $R$, acts as a convolution with a Gaussian line profile of width $\sigma=\frac{c}{2.355 R}$.  Real astronomical instruments usually have wavelength-dependent resolving power, which complicates the implementation.  The extent to which this effect matters will depend on the science case.  For now, \texttt{blas\'e} simply assumes a fixed resolving power.

The data-pixel sampling is much coarser than the model pixel sampling.  This resampling step could be achieved with two choices.  One would be to simply evaluate the closed-form model at the coordinates of the data pixels $\bm{\lambda_D}$.  This choice would require a closed-form solution to Equation \ref{eqn_convolution}, which has recently been reported \citep{2021arXiv211006271L}.  Instead, we choose the conventional sampled approach, in which we evaluate the extrinsic model at all of the native resolution wavelength coordinates, and then compute the mean value of those pixels within the bounds of each coarse data pixel.  The resampling procedure is autodiff-aware: the same clusters of high resolution coordinates map to the same data pixel coordinates, no matter what the $RV$ is.  The $RV$ only dictates what flux values are realized within those pixels.  The final forward model for \emph{blas\'e} is designated simply as $\mathsf{M}$ without subscripts to emphasize that we have achieved the desired goal of a plausible model for each datum in the 1D observed spectrum:

\begin{eqnarray}
    \mathsf{M}(\bm{\lambda}_D) = \resample{\Big[\mathsf{M}_{\rm joint}(\bm{\lambda}_Z) * g(R) \Big]} \label{eqn_final_model}
\end{eqnarray}

where $g$ is the Gaussian instrumental convolution kernel and the \texttt{resample[]} operation indicates the average of model pixels that fall within each data pixel's red and blue boundaries.

\subsection{Regularization}

Equation \ref{eqn_final_model} has up to $N_{\mathrm{lines}}\times3 \sim20,000$ tunabale parameters from the star, $N_{\mathrm{lines}}\times3 \sim15,000$ tunabale parameters from the Earth's atmosphere, plus $v\sin{i}$ and $RV$.  That adds up to 35,002 model parameters.  The resolving power may also be treated as tunable if it is not known or varies slightly with \emph{e.g.} seeing, slit-or-fiber illumination, or instrumental configuration.

It may appear desirable to simply optimize all of these parameters in a \emph{laissez-faire} manner, allowing them all to take on whatever value the data dictates.  Such a stratagem would overfit the data, resulting in unphysically perverse lineshapes that do not reflect the air of reality we attempt to impose on our synthetic spectral models.  Lines would haphazardly fit noise-spikes, and conspire together to warp spectral shapes in unexpected ways.  This overfit model may suit some rare purposes.  But most of the time, we prefer to strike a better balance in the bias-variance tradeoff.

We apply some amount of \emph{regularization}, a restiction on the allowed values the model parameters can take on.  Fortunately we have a firm theoretical basis to justify this regularization.  We believe our precomputed synthetic spectral models are \emph{quasi-statically correct}: the predicted spectra resemble the unobserved ``latent spectrum'' with lines in the correct place, but just with the wrong amplitudes.  This statement may stem from the fact that it is easier to predict the mere existence of some energy transition of atoms and molecules than it is to predict their transition rates, abundances, temperature and pressure effects, and all the other line strength effects that flow down to how much light a line ultimately absorbs in a stellar atmosphere.

Even still, the regularization step is in some ways the most subjective, since the degree of regularization will control the degree of overfitting or underfitting. The most extreme regularization---the antithesis of the \emph{laissez-faire} scenario---would yield a model too rigid to respond to the data at all, yielding a model entirely unchanged from the cloned PHOENIX and \texttt{TelFit} models.  So regularization constitutes the only hyperparameters worthy of tuning in \texttt{blas\'e}.  The choice of how to set the regularization is problem-specific.  We default to the following choice. We fix all line parameters except for amplitude, which receives a regularization of 0.01, meaning that its ampltidue should be within about 1\% of its cloned value.  The extrinsic $v\sin{i}$ and $RV$ have no regularization, but in practice they need to be initialized close to their plausible values.

Figure \ref{fig_WASP69_demo} shows an example of comparing HPF data to the closest PHOENIX template, post-processed to the HPF resolution and sampling.  Figure \ref{fig_WASP69_transferred} shows the same data spectrum compared to a pixel-level \emph{blas\'e} model regularized to the clone in Figure \ref{fig_cloned_spectrum_demo}.  The latent high-resolution model for WASP 69 is therefore the transfer-learned model, unadjusted for the extrinsic and instrumental properties.  This semi-empirical model is shown in Figure \ref{fig_WASP69_regularized}.


\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/blase_WASP69_demo.png}
    \caption{WASP 69 observed with HPF, compared to a $T_{\mathrm{eff}}=4700\;$K,  $\log{g}=4.5$ solar metallicity PHOENIX model warped to $v\sin{i}=2.2$~km/s, $RV=-9.6$~km/s, and HPF resolving power.}
    \label{fig_WASP69_demo}
\end{figure}

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/blase_WASP69_regularized.png}
    \caption{Semi-empirical model of WASP 69 transfer-learned with \texttt{blas\'e}, employing a regularization prior on the learned amplitudes.}
    \label{fig_WASP69_transferred}
\end{figure}

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/blase_super_resolution_template.png}
    \caption{The native-resolution semi-empirical model transfer-learned from WASP 69 HPF data. The revised model can be viewed as a super-resolution deconvolution of the HPF spectrum.}
    \label{fig_WASP69_regularized}
\end{figure}


% \begin{outline}
%     \1 What is the residual level with bare PHOENIX (i.e. tuning continuum only, not lines)?
%     \1 What is the residual level with no regularization (i.e. tune all lines and continuum)?
%     \2 Should be near-zero except for missing lines
%     \2 What should we do about missing lines?
%     \1 What is the residual level with modest regularization? What is the typical change to the line?
%     \2 Plot of cloned FWHM versus FWHM transferred
%     \2 The pseudo-Voigt line properties can be analytically integrated to give an equivalent width
%     \2 Plot of cloned EW (before) versus transfer EW (after)
% \end{outline}

%\section{Injection/recovery tests}\label{secInjRec}
%Here we conduct injection/recovery tests.

\section{Discussion}\label{secDiscuss}

\subsection{Comparison to existing frameworks}

Several astronomical spectral frameworks share similar aims as \texttt{blas\'e}.  These existing frameworks will have enduring value for the wide range of problems in the field of stellar spectroscopy.  Here we scrutinize the differences among some of these approaches to clarify how This Work fits in.

The \texttt{specmatch} synthetic template matching tool produces noise-free nearest neighbor templates given an input spectrum \citep{2015PhDT........82P}.  Several practical barriers limit the accuracy of using precomputed synthetic spectral models alone. First and foremost, real stars are usually more complicated than our simplified models of them. Real spectra often vary over more dimensions that our models do.  Conspicuous examples of these hidden variables can be found in protostars: starspots, accretion veiling, dust extinction, and magnetic Zeeman splitting. Jointly modeling all of these phenomena alongside the intrinsic stellar photosphere is challenging.

The empirical version, \texttt{specmatch-emp} \citep{2017ApJ...836...77Y} matched spectra better than the synthetic templates, but is still too rigid for some applications and requires the assembly of hundreds of standardized high signal-to-noise-ratio templates, ideally with low intrinsic rotational broadening.  Such a large number of high-quality templates with high resolving power and low $v\sin{i}$ has not yet been established in the near infrared.

The \texttt{wobble} framework \citep{2019AJ....158..164B} modernized the construction of high-SNR templates to account for temporally variable telluric lines. The tool requires dozens of high-SNR spectra acquired at a range of Barycentric Earth Radial Velocities (BERVs).  The final telluric-free combined spectrum would still have to be compared to models for absolute calibration, or can be used out-of-the-box for precision relative RVs.  The \texttt{wobble} framework also pioneered the off-label application of automatic differentiation frameworks---in this case \texttt{TensorFlow}---towards their physically-motivated use in stellar spectra.  \texttt{blas\'e} can be viewed as an evaluable and interpretable super-resolution version of \texttt{wobble}, that accepts more bias in the bias-variance tradeoff.

The \texttt{starfish} framework \citep{czekala15} provides a robust likelihood function for data-model comparisons, and retires many of the problems in this domain.  \texttt{starfish} pioneered the use of whole-spectrum fitting with resilience to model imperfections by addressing the problem of what to do when the underlying atomic and molecular data was wrong or approximate or missing.  It has been extended to inferring starspot physical properties \citep{2017ApJ...836..200G}, measuring veiling in Class 0 protostars \citep{2018ApJ...862...85G}, and quantifying imperfections in brown dwarf models \citep{2021ApJ...921...95Z}.  The Spectral Inference Crank \citep[\texttt{sick},][]{2016ApJS..223....8C} shares similar aims as \texttt{starfish}, and provides additional useful grid search capabilities.

For very large bandwidths and very many spectral lines, the problem of identifying and cataloging line imperfections essentially becomes a book-keeping and continuum assignment problem.  \texttt{blas\'e} and \texttt{starfish} provide different strategies for orchestrating the line-mismatch identification procedure, with each route having tradeoffs depending on the application.


\section{Conceivable Improvements, Extensions, and Limitations}

As currently implemented, \texttt{blas\'e} represents a minimum viable product: it out-performs some existing approaches for some science cases.  The scaffolding of the framework is designed in a way to promote extensibility, so the future of the framework is bright.  Here we enumerate some planned or conceivable extensions to the framework that would unlock new science arenas.  We order the presentation of these extensions from our perceived ease-of-implementation, from easiest to hardest.

\subsection{Exact instead of pseudo Voigt Profile}
We currently employ the pseudo-Voigt profile for its low computational cost.  We have a prototype exact-Voigt-Hjertingimplementation following \citet{2022ApJS..258...31K}.  We coarsely estimate that moving to this exact-Voigt implementation wculd decrease some residual regions by $\sim 30\%$, while increasing the computational cost by more than $10\times$ over the existing pseudo-Voigt approximation.  The exact-Voigt-Hjerting implementation still outbids the higher cost of a direct numerical convolution of a Gaussian and Lorentzian profile.



\subsection{Addressing the pseudo-continuum with Gaussian Process regression}

We currently assume the input spectra are perfectly normalized to the continuum.  This approximation impacts all training stages, including the stellar cloning, telluric cloning, and data-model comparison.  Gaussian Processes (GPs) could and should be brought to bear at each of these optimization stages, as described in the ``global kernels'' in \texttt{Starfish} \citet{czekala15}.  In short, a GP-likelihood relaxes the assumption that the continuum has been perfectly normalized, in favor of the more realistic statement ``the continuum has been coarsely normalized, with some characteristic-but-as-yet-unknown correlation and scale length and amplitude of the imperfections''.  That statement translated to the following modification to Equation \ref{simpleLikelihood}:

\begin{eqnarray}
    \mathcal{L} =  \frac{1}{2}\mathsf{R^\intercal} \mathsf{C}^{-1} \mathsf{R} +\frac{1}{2}\ln{\det{\mathsf{C}}} \label{GPLikelihood}
\end{eqnarray}

where we introduce the covariance matrix $\mathsf{C}$, with its concommittant kernel and collection of typically 2-3 parameters.  We anticipate that this GP likelihood would have the greatest impact on stellar stars with significant band-heads and line-blanketing: spectra with a so-called ``pseudo-continuum''.  M-dwarfs and brown dwarfs fall into this fear-inducing category.

The main demerit of moving to a GP-likelihood is computational cost.  Fortunately a few efficient autodiff-aware implementations of GPs exist. The \texttt{celerite} algorithm \emph{celerit\'e} \citep{2017AJ....154..220F} has an exact backpropation implementation \citep{2018RNAAS...2...31F} that scales linearly with the number of data points.  The celerite algorithm does not currently have a PyTorch implementation.  The \texttt{GPyTorch} framework \citep{2018arXiv180911165G} has a large category of approximate and exact GPs that could be straight-forwardly dropped into \texttt{blas\'e}.  Even still these GPs could increase the computation cost by of order $10\times$.

An alternative to GPs could be to simply tune the $\mathsf{P}$ term that represents the wavelength-dependent pre-factor to Equation \ref{equation1}.  Tuning $\mathsf{P}$ would correct for only large-scale imperfections in the otherwise-fixed continuum flattening procedure.


\subsection{Minibatches and Stochastic Gradient Descent}

Currently each training epoch ``sees'' the entire dataset, a setup dubbed ``full-batch'' gradient descent.  An alternative scheme allow training with only a portion of the entire dataset at a time in ``minibatches''.  The massive data volumes in modern Neural Network applications cannot fit into the GPU memory, and so minibatches are a necessity.  Our meager $1$ MB dataset can easily fit into the GPU memory, but our model can be large if we have a large number of pixels or lines or both.  So while minibatches may not be required due to data size limitations they may be useful for particularly large models.  Minibatches also act as a form of regularization, the principal source of stochasticity in the Stochastic Gradient Descent algorithm, which tends to have better convergence than full-batch Gradient Descent \citep{2016arXiv160904747R}.

We experimented with minibatches by assembling and evaluate only a portion of the dense $\bar{\bm{F}}$ matrix at a time, in minibatches. The choice to evaluate only a portion of lines at a time would mean the model is inaccurately evaluated at all wavelength pixels. Instead, we choose to evaluate all lines, but only on a random subset $N_{\mathrm{batch}}$ of the total pixels $N_s$, so that the model can eventually converge to exact at those points. All lines are allowed to update at each glimpse of a minibatch, but many lines with cores far from minibatch pixels will provide only weak information about how the loss scalar changes for their parameters. The value of $N_{\mathrm{batch}}$ should be set just below the threshold at which the computation runs out of memory. At present, this threshold is determined experimentally. The indices of the wavelength points can either be set to be contiguous in blocks (duplicating the sparse matrix approach), or random with possibly large gaps between indices, or else with some other scheme tailored to the most informative wavelength regions. We experimented with several techniques and settled on random sampling with replacement, meaning that some pixels get revisited more often than others, with some minibatches tailored only to the line cores.  Overall minibatches as implemented above performed worse than the sparse implementation, with both lower accuracy and slower computation time.

\subsection{Broad lines and advanced lineshapes}

Some lines, such as Hydrogen, sodium, potassium, and others have extremely broad line wings, approaching larger than the $\sim6000$ pixels we allocate for the sparse implementation. These special lines should be handled separately from the weak lines, both from a computational performance perspective and an accuracy perspective.

Extremely broad lines will exhibit truncation effects if the sparse window is small compared to the line wing size. The truncation effects will look like tophat functions severing the asymptotic wings, imbuing artificial step function kinks in the emulated spectrum. We can afford to increase the sparse window on a few, say $N_{broad}\sim20$ of the broadest spectral lines. We then construct and evaluate the entire dense matrix for those lines: $\sim 330\;000 \times 20$. The number of FLOPS in each category scales as about 6 Million for the 20 broad and dense lines versus about 36 Million for the sea of about 6500 narrow and sparse lines, depending on the exact choices for wing cuts and number of lines.

One could introduce advanced lineshapes for these $\sim20$ broad lines, perturbing the Voigt line-wings with a smooth wavelength-dependent correction term $\mathsf{G}$:

\begin{eqnarray}
    \mathsf{\tilde{V}(\bm{\lambda})} &=& \mathsf{V} \odot \mathsf{G}\\
    \mathsf{G} &=& 1 + (e^{a_j} - 1) \cdot \mathcal{S}\left(\frac{|\bm{\lambda}-\lambda_{c,j}| - \lambda_{t, j}}{b_j}\right)
\end{eqnarray}

where $\odot$ is again the element-wise product (\emph{a.k.a} Hadamard product), $\mathcal{S}$ is the sigmoid function, and we have introduced three new tunable parameters for each of the $j$ broad lines; $\lambda_t$ is the truncation wavelength, $b$ is a scale parameter for how slowly or how rapidly in wavelength-space the transition from non-Lorentzian proceeds, and $a$ is a possibly negative stretch parameter that controls whether the line wing is sub- or super- Lorentzian.

This functional form has a few advantages. It is smooth. The smoothness of the transition is controlled by a tunable parameter, $b$. It can handle either sub- or super-Lorentzian shapes. It is roughly based on the $\chi$-factor\authorcomment1{citation to Hartmann et al. 1989, etc.}. In the limit $\lim_{a\to0} \mathsf{G}$, the lineshape becomes exactly Lorentzian. The sigmoid is efficiently implemented in PyTorch.
It enforces that the perturbation only produces absorption and not emission profiles.

\subsection{Wavelength dependent limb darkening}
Currently the extrinsic model step possesses up to four parameters: the $v\sin{i}$ and $RV$, and 2 optional parameters for limb-darkening.  These four parameters may adequately parameterize a star with a uniform stellar disk.  Extremely high signal-to-noise-ratio spectra of rapidly rotating stars may require additional flexibility.  The limb darkening is generally wavelength dependent, and so a pan-chomatic spectrum may require a different limb-darkening from the blue end to the red end.  The limb darkening may instead depend on physical properties of the spectral line formation, such as physical depth of formation, and so the extent of limb-darkening may jump haphazardly from line-to-line-to-line, rather than as a predictably smooth function across wavelength.  \texttt{blas\'e} could be built to handle such a seemingly pathological scenario by adding a vector of limb darkening parameters, one for each line.  One would have to regularize the fits with some typical limb darkening and a heuristic penalty for departures from this mean.

\subsection{Doppler Imaging}
The fixed $v\sin{i}$ approximation breaks down for stars with large-scale surface features.  Doppler imaging \authorcomment1{xx, yy, zz} attempts to reverse engineer the surface map from the extent to which observed line profiles depart from a pristine rotational broadening kernel.  This reverse engineering step suffers from a vast number of geometrical degeneracies, but still provides useful constraints on stellar surfaces.  Following LugerXX, we emphasize a distinction between A) longitudinally symmetric surface features, and B) longitudinally asymmetric surface features.  Most radial velocity practitioners think about the latter, since longitudinally asymmetric surface features imbue skewness to the line profiles, causing radial velocity perturbations easily detectable in radial velocity time series.  These confound exoplanet searches.

Longitudinally symmetric surface features to not change as the star rotates on its axis.  The existence of these features change the \emph{kurtosis} of the spectral line.  For example a hypothetical non-emitting (black) polar starspot exhibits a deficit in flux at the line core, resulting in less zero-velocity flux than its homogeneous counterpart.  A dark zonal band results in equal-sized bites out of the red- and blue- sides of the line.

It is much easier to reverse-engineer longitudinally asymmetric features than longitudinally symmetric ones.  The latter requires exact knowledge of the underlying spectral template.  Isolated deep spectral lines constitute the only practical scenario where exact knowledge can be claimed.  Isolated spectral lines may be scarce or absent for M-dwarfs and brown dwarfs where lines blend together ostensibly in an inseparable way, counfounding Doppler imaging.

\texttt{blas\'e} offer a new approach to Doppler imaging that may overcome these historical limitations.  In \texttt{blas\'e} can...


\subsection{RV jitter and microturbulence}

Already \texttt{blas\'e} is equipped to fit every single line with its own systemic $RV$, by tuning the line center position $\lambda_\mathrm{c}$ at each training epoch.  There exists both empirical evidence and some theoretical motivation that $RV$ jitter varies from line-to-line \authorcomment1{Xavier Dumusque 2018}.  Importantly, the extent of this line-by-line RV jitter could be predicted in part by the depth of line formation.  A future extreme-precision-RV (EPRV) version of \texttt{blase} could leverage this information in a few ways.  As an example, the depth-of-formation for all or a subset of lines could be obtained and associated with each spectral line.  A regulariztion scaling term could be introduced to allow the line positions to vary, but only in proportion to their depth of formation.


\subsection{Prospects for closed-form computation of Jacobians}
In principle, one could compute the Jacobians analytically---since the partial derivatives of the Voigt function are known in closed-form.  This analytical approach would cut-out the autodiff approach, using the exact Jacobians to compute the Gradient Descent step.  The analytical approach may be faster and/or possess lower memory overhead than the autodiff one.  However, working out the chain-rule for the closed-form analytical Jacobians may become difficult or impossible once combined with the many augmentations get applied at the joint telluric-and-stellar modeling step.  So analytic Jacobians could principally accelerate the cloning step, but would add significant implementation complexity at the important data-model comparison step.  We therefore deem this extension somewhat diminishing returns on scarce developer time.


\subsection{Lineshapes choice and tradeoffs}
\begin{outline}
    \1 Faced with a question: what lineshape to use
    \2 To First order the lines are Gaussian or Lorentzian
    \2 To Second-order the lines are Voigt Profiles
    \2 In detail the lines are the result of radiative transfer that can smear the Voigt profile
    \2 In practice the high-res lines get convolved with instrumental profiles, so details don't matter too much
    \1 Computational tradeoff of Lorentzian/Gaussian versus Pseudo-Voigt versus Voigt
    \2 Additional Implementation challenge: Fadeeva function not implemented in PyTorch
    \1 We choose a pseudo-Voigt as a balance between computation and adequate accuracy
\end{outline}

\subsection{Questioning the assumptions of the method}

\begin{outline}
    \1 Is PseudoVoigt Adequate?
    \1 What performance level is adequate? (depends on your application)
    \1 What about PRV applications
    \1 Are we using the GPU effectively? Are we memory bandwidth limited?
    \1 Should we update the line center positions?
\end{outline}

\subsection{Limitations}
\begin{outline}
    \1 What to do when continuum is absent even at the native resolution of precomputed synthetic model?
    \1 Relevant to brown dwarfs and possibly M dwarfs molecular bands
    \1 Bandwidth limitations
    \1 Interplay with tellurics
\end{outline}



\section{Conclusions}
More placeholder text...


\

\begin{acknowledgements}
    This material is based upon work supported by the National Aeronautics and Space Administration under Grant Numbers 80NSSC21K0650 for the NNH20ZDA001N-ADAP:D.2 program, and 80NSSC20K0257 for the XRP program issued through the Science Mission Directorate.  We acknowledge the National Science Foundation, which supported the work presented here under Grant No. 1910969.  This research has made use of NASA's Astrophysics Data System Bibliographic Services.
\end{acknowledgements}

\clearpage


\facilities{HET (HPF)}

\software{ pandas \citep{mckinney10},
    matplotlib \citep{hunter07},
    astropy \citep{exoplanet:astropy13,exoplanet:astropy18},
    exoplanet \citep{exoplanet:joss}, %celerite?
    numpy \citep{harris2020array},
    scipy \citep{2020SciPy-NMeth},
    ipython \citep{perez07},
    starfish \citep{czekala15},
    seaborn \citep{Waskom2021},
    pytorch \citep{2019arXiv191201703P}}


\bibliography{ms}


\clearpage

\appendix
\restartappendixnumbering

\section{Log flux scaling mode} \label{appendixLogScale}

Here we illustrate how \texttt{blas'e} gets altered when applying the logarithmic flux pre-processing step.  First, we compute the natural log of the flux directly on the precomputed synthetic spectrum in its absolute flux scaling and native pixel sampling:

\begin{eqnarray}
    \ln{\mathsf{S}} = \ln{\mathsf{S}_{\rm abs}} - \ln{\mathsf{B}} - \mathsf{P}
    \label{eqnlogFlat}
\end{eqnarray}

We simply ``rebrand'' $\mathsf{P}$ as residing in logarithmic flux units, and disregard it since it is largely a nuisance parameter anyways.  We then treat the \texttt{blas\'e} clone model as a sum of opacities, retaining the Voigt profile:

\begin{eqnarray}
    \ln{\mathsf{S}_{\rm clone}} = -\sum_{j=1}^{N_{\mathrm{lines}}} a_j \mathsf{V}_j \label{equationOpacitySum}
\end{eqnarray}

Here, the $a_j$'s have also been slightly rebranded from their meaning in Equation \ref{equation1}.  We still want to enforce only absorption lines---and not spurious emission lines---so we use the sample trick of sampling the $a_j$'s in log and then exponentiating them to get guaranteed positive values.  Note that Equations \ref{equation1} and \ref{equationOpacitySum} carry modified meanings for the Voigt profile.  Specifically, Equation \ref{equation1} can be viewed as the Taylor Series expansion for \ref{equationOpacitySum} in the limit of small opacities:

\begin{eqnarray}
    e^{-a_j \mathsf{V}_j} \approx (1-a_j\mathsf{V}_j) \label{eqnTaylor}
\end{eqnarray}

Both equations are approximate. A real stellar atmosphere's lineshape arises from a sum of disparate Voigt profiles weighted along a nonuniform column of gas, whereas here we have assumed the column of gas is approximated as a single uniform isothermal backlit layer.  A sum of unlike-Voigt profiles is not exactly equal to any single Voigt profile.  Theoreticians may resonate with this more ``first principles'' representation, while data practitioners may find Equation \ref{equation1} more natural, so to some extent the choice is a matter of taste.

The sparse matrix gets rebranded as filled with opacity values, instead of log-fluxes, but operationally remains the same. All subsequent steps operate on the summed-and-exponentiated opacities, behaving identically to their linear counterparts.  For example, we exponentiate before computing the residuals and data-model comparison, $\mathsf{R} = e^{\ln{\mathsf{S}}} - e^{\ln{\mathsf{S}_{\rm clone}}}$.



% Table
\begin{deluxetable}{cp{12cm}}
    \tablecaption{Notation used in this paper\label{table2}}
    \tablehead{
        \colhead{Symbol} & \colhead{Meaning}
    }
    \startdata
    \hline
    \multicolumn{2}{c}{Spectra}\\
    \hline
    $\bm{\lambda}_S$ & Native wavelength coordinates of the precomputed stellar spectrum\\
    $\bm{\lambda}_T$ & Native wavelength coordinates of the telluric spectrum\\
    $\bm{\lambda}_D$ & Native wavelength coordinates of the data spectrum\\
    $\mathsf{S}_{\rm abs}$ & Flux values of the precomputed synthetic stellar spectral model $\bm{\lambda}_S$\\
    $\mathsf{B}$ & Blackbody of temperature $T_{\mathrm{eff}}$ to coarsely normalize $\mathsf{S}_{\rm native}$\\
    $\mathsf{P}$ & Smooth polynomial to refine continuum-normalization\\
    $\mathsf{S}$ & Continuum normalized augmentation of $\mathsf{S}_{\rm abs}$\\
    $\mathsf{T}$ & Transmission values of the precomputed synthetic telluric model \\
    $\mathsf{D}$ & The observed data spectrum flux values\\
    $\bm{\epsilon}$ & The estimated uncertainties in the data spectrum\\
    $\mathsf{S}_{\rm clone}$ & Evaluable and tunable cloned flux model of $\mathsf{S}$\\
    $\mathsf{T}_{\rm clone}$ & Evaluable and tunable cloned transmission model of $\mathsf{T}$\\
    $\mathsf{S}_{\rm ext}$ & An augmentation of $\mathsf{S}_{\rm clone}$ with $v\sin{i}$ convolution and $RV$ translation\\
    $\mathsf{M}_{\rm joint}$ & The joint stellar and telluric model: $\mathsf{S}_{\rm ext} \odot \mathsf{T}_{\rm clone}(\bm{\lambda}_S)$  \\
    $\mathsf{M}$ & Joint model convolved
    with instrumental kernel and resampled to $\bm{\lambda}_D$\\
    $\mathsf{R}$ & The residual spectrum between a pair of inputs, \emph{e.g.} $\mathsf{D} - \mathsf{M}$\\
    $\bm{v}$ & The spectral coordinate axis $\bm{\lambda}$ expressed as a velocity difference\\
    \hline
    \multicolumn{2}{c}{Line properties}\\
    \hline
    $\lambda_{\mathrm{c},j}$ & Line center position of the $j^{th}$ spectral line\\
    $a_j$ & Gaussian line profile amplitude of the $j^{th}$ spectral line \\
    $\sigma_j$ & Gaussian line profile scale of the $j^{th}$ spectral line\\
    $\gamma_j$ & Lorentzian line profile half width of the $j^{th}$ spectral line\\
    $\mathsf{V}_j$ & The Voigt profile of the $j^{th}$ spectral line \\
    $\bar{\bm{F}}$ & The dense $(N_{\rm lines} \times N_{x})$ matrix of all line fluxes stacked vertically \\
    $\hat{\bm{F}}$ & The sparse $(N_{\rm lines} \times N_{\rm sparse})$ matrix of all line fluxes stacked vertically \\
    $\zeta$ & The rotational broadening convolution kernel\\
    $g$ & The instrumental broadening convolution kernel, typically a Gaussian\\
    \hline
    \multicolumn{2}{c}{Scalars}\\
    \hline
    $N_{\rm lines}$ & Number of spectral lines \\
    $N_{x}$ & Number of pixel coordinates in the precomputed spectrum $\bm{\lambda}_x$\\
    $N_{\rm sparse}$ & Number of non-zero pixels computed in the sparse implementation\\
    $\pm \Delta \lambda_{\mathrm{buffer}}$ & Buffer exceeding the red and blue limits of the data spectrum\\
    $P_{\rm rom}$ & The prominence threshold of spectral lines to include in cloning \\
    $v\sin{i}$ & Rotational broadening for stellar inclination $i$ and equatorial velocity $v$\\
    $RV$ & Radial velocity of the star\\
    $R$ & Spectrograph resolving power $\lambda/\delta\lambda$\\
    $\mathcal{L}$ & The loss scalar, usually the sum of the squares of the residuals\\
    \hline
    \multicolumn{2}{c}{Operators}\\
    \hline
    $\resample \big[ \mathsf{F(\bm{\lambda}_x)} \big]$ & The resample operator, takes in a flux spectrum $\mathsf{F}$ evaluated at $\bm{\lambda}_x$ coordinates and returns the mean flux within the pixel boundaries of coordinate $\bm{\lambda}_z$\\
    $*$& The convolution operator\\
    $\odot$& \emph{Hadamard product}, an elementwise product of two same-length vectors\\
    \enddata
\end{deluxetable}

\end{document}

