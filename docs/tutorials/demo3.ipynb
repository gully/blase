{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aca5d70",
   "metadata": {},
   "source": [
    "# (Aside) Loading a pretrained blasé model\n",
    "\n",
    "Here we demonstrating loading a pretrained model with blasé and provide some guidance on how to think about such large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ddecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from blase.emulator import SparseLinearEmulator, SparseLogEmulator\n",
    "from blase.utils import doppler_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f49397",
   "metadata": {},
   "source": [
    "### A philosophical aside on saving large models\n",
    "by gully\n",
    "\n",
    "This practice of saving large models and reproducing them later may seem unsual to newcomers from physical science.  The \"lookup table\" of this saved model state is called a **pretrained model** in the Machine Learning community.  There are [many](https://huggingface.co/models) [websites](https://modelzoo.co) where you can download pretrained models for all sorts of applications in artificial intelligence.  \n",
    "\n",
    "You can think of this process as fitting $\\vec{y} = m \\vec{x} + b$.  Instead of saving the raw data $\\vec{y}$ and their coordinates $\\vec{x}$, you can compress the voluminous information in  the data into a small lookup table of values $(m,b)$.  In this case, the lookup table of the model state has only 2 entries.  Astronomers are used to model states with typically a few dozen or fewer parameters.\n",
    "\n",
    "Machine learning models---including those in `blasé`---are merely big versions of $(m,b)$.  A pretrained model in `blasé` will likely possess 30,000+ entries.  Google recently trained a [540 billion parameter model](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).  \n",
    "\n",
    "In addition to the model state, you *also* need the **architecture** or \"functional form\" of the model.  In other words, knowing $(m,b)$ will do you no good if you compute $b \\vec{x} - m$.  The output would yield garbage (unless $m$ and $b$ happened to be zero).\n",
    "\n",
    "So pretrained models should also come with a note, explaining exactly what version of the code was used to produce the model.  If someone changed the code since the \"lookup table\" was generated, the reproduced model may fail.  The fail may be silent or loud.\n",
    "\n",
    "This \"stale\" pretrained model problem poses a tension in an evolving framework like `blasé`.  As I edit the code to improve it, any existing pretrained models are liable not to work anymore: the underlying functional form may have changed.  \n",
    "\n",
    "Therefore for best results, we currently recommend that you don't trust old pretrained models too much, and instead re-generate your clone soon before you transfer it to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf52d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_lo = 11_000-60\n",
    "wl_hi = 11_180+60\n",
    "wavelength_grid = doppler_grid(wl_lo, wl_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be917f63",
   "metadata": {},
   "source": [
    "We can now use any wavelength grid we like!  We choose a uniform-in-velocity-spacing grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stellar_pretrained_model = torch.load('phoenix_clone_T4700g4p5_prom0p01_11000Ang.pt')\n",
    "telluric_pretrained_model = torch.load('telfit_clone_temp290_hum040_prom0p01_11000Ang.pt')\n",
    "print(stellar_pretrained_model['lam_centers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cafd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "stellar_emulator = SparseLinearEmulator(wavelength_grid, \n",
    "                                        init_state_dict=stellar_pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbcc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "telluric_emulator = SparseLogEmulator(wavelength_grid, \n",
    "                                        init_state_dict=telluric_pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e9a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    stellar_flux = stellar_emulator.forward().cpu().numpy()\n",
    "    telluric_transmission = telluric_emulator.forward().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.step(wavelength_grid, stellar_flux, label='Pretrained stellar model')\n",
    "plt.fill_between(wavelength_grid, telluric_transmission, 1.0,\n",
    "                label='Pretrained telluric model absorption', alpha=0.1, color='k')\n",
    "plt.xlim(11_080, 11_090)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47016899",
   "metadata": {},
   "source": [
    "Voilá! We successfully read-in our pretrained models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
